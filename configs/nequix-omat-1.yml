resume_from: "checkpoints/nequix-omat-1.pkl"
state_path: "checkpoints/nequix-omat-1.pkl" # for saving/resuming training

cutoff: 6.0

# dataset specific
train_path: "data/omat/train"
valid_path: "data/omat/val"

atomic_numbers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]

# isolated atom energies (eV)
# copied from https://github.com/facebookresearch/fairchem/blob/ad6f494/configs/uma/training_release/element_refs/iso_atom_elem_refs.yaml#L203C1-L303C6
atom_energies:
  1:  -1.11700253
  2:  0.00079886
  3:  -0.29731164
  4:  -0.04129868
  5:  -0.29106192
  6:  -1.27751531
  7:  -3.12342715
  8:  -1.54797136
  9:  -0.43969356
  10: -0.01250908
  11: -0.22855413
  12: -0.00943179
  13: -0.21707638
  14: -0.82619133
  15: -1.88667434
  16: -0.89093583
  17: -0.25816211
  18: -0.02414768
  19: -0.17662425
  20: -0.02568319
  21: -2.13001165
  22: -2.38688845
  23: -3.55934233
  24: -5.44700879
  25: -5.14749562
  26: -3.30662847
  27: -1.42167737
  28: -0.63181379
  29: -0.23449167
  30: -0.01146636
  31: -0.21291259
  32: -0.77939897
  33: -1.70148487
  34: -0.78386705
  35: -0.22690657
  36: -0.02245409
  37: -0.16092396
  38: -0.02798717
  39: -2.25685695
  40: -2.23690495
  41: -2.15347771
  42: -4.60251809
  43: -3.36416792
  44: -2.23062607
  45: -1.15550917
  46: -1.47553527
  47: -0.19918102
  48: -0.01475888
  49: -0.19767692
  50: -0.68005773
  51: -1.43073368
  52: -0.65790462
  53: -0.18915279
  54: -0.01179476
  55: -0.13507902
  56: -0.03056979
  57: -0.36017439
  58: -0.86279246
  59: -0.20573327
  60: -0.2734463
  61: -0.20046965
  62: -0.25444338
  63: -8.37972664
  64: -9.58424928
  65: -0.19466184
  66: -0.24860115
  67: -0.19531288
  68: -0.15401392
  69: -0.14577898
  70: -0.19655747
  71: -0.15645898
  72: -3.49380556
  73: -3.5317097
  74: -4.57108006
  75: -4.63425205
  76: -2.88247063
  77: -1.45679675
  78: -0.50290184
  79: -0.18521704
  80: -0.01123956
  81: -0.17483649
  82: -0.63132037
  83: -1.3248562
  89: -0.24135757
  90: -1.04601971
  91: -2.04574044
  92: -3.84544799
  93: -7.28626119
  94: -7.3136314

avg_n_edges: 736.2363228968411
avg_n_neighbors: 39.200198903821516
avg_n_nodes: 18.68197878523378
max_n_edges: 17940
max_n_nodes: 236
# scale: 2.836595315213642
# scale is calculated from MPtrj with esoteric-ephemera isolated atom energies
scale: 0.8080419656942678
# shift is calculated from OMAT with OMAT isolated atom energies
shift: -3.513482726416955

# model specific
hidden_irreps: "128x0e + 64x1o + 32x2e + 32x3o"
lmax: 3
n_layers: 4
radial_basis_size: 8
radial_mlp_size: 64
radial_mlp_layers: 2
radial_polynomial_p: 6.0
mlp_init_scale: 4.0
index_weights: false
layer_norm: true
kernel: True

# training specific
optimizer: "muon"
learning_rate: 0.01
warmup_epochs: 0.1
warmup_factor: 0.2
grad_clip_norm: 100.0
weight_decay: 1.0e-3
batch_size: 128  # NOTE: per device (used 4 GPUs for training)
n_epochs: 6
energy_weight: 20.0
force_weight: 20.0
stress_weight: 5.0
loss_type: "mae"
log_every: 100
ema_decay: 0.999
